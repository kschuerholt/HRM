# Augmentation ablation experiments
# Tests different levels of data augmentation

defaults:
  - /arch: hrm_v1
  - _self_

# Data path will be overridden by experiment runner
data_path: data/arc-aug-1000  # Default, will be overridden

# Training hyperparameters (standard config)
global_batch_size: 768
epochs: 30000  # Shorter than baseline for ablation
steps_per_epoch: 100
eval_every_n_epochs: 200
save_every_n_epochs: 500

# Learning rates
lr: 1e-4
lr_min_ratio: 1.0
lr_warmup_steps: 2000

# Optimizer settings
beta1: 0.9
beta2: 0.95
weight_decay: 0.1
puzzle_emb_weight_decay: 0.1

# Puzzle embedding learning rate
puzzle_emb_lr: 1e-2

# Data
max_seq_len: 900

# Optimizer
optim:
  name: torch.optim.AdamW
  betas: [0.9, 0.95]
  eps: 1e-8

# Scheduler  
scheduler:
  name: transformers.get_cosine_schedule_with_warmup
  num_warmup_steps: 2000
  num_training_steps: 3000000  # epochs * steps_per_epoch

# Logging and monitoring (will be overridden)
run_name: augmentation_ablation
wandb:
  project: hrm_experiments
  name: augmentation_ablation
  
# Output
output_dir: outputs/augmentation_ablation

# Evaluation settings
eval_split_names: ["test"]
return_keys: ["q_halt_logits", "q_continue_logits"]

# Experiment specific
seed: 42